{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06becde8",
   "metadata": {},
   "source": [
    "## **🧹 NLPTA: Amharic Text Cleaning, Tokenization & Stopword Removal Demo** ##\n",
    " \n",
    "\n",
    " A step-by-step walkthrough of preprocessing real Amharic text.\n",
    " \n",
    " **This notebook demonstrates:**\n",
    " - Loading sample Amharic text from Wikipedia\n",
    " - Cleaning punctuation and whitespace\n",
    " - Tokenizing into words\n",
    " - Removing stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a914ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspaces/nlpta\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/workspaces/nlpta\")\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "from nlpta import load_sample_corpus, clean_text, tokenize, remove_stopwords, load_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea0751a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading sample Amharic corpus...\n",
      "✅ Loaded 4 paragraphs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"📥 Loading sample Amharic corpus...\")\n",
    "corpus = load_sample_corpus()\n",
    "print(f\"✅ Loaded {len(corpus)} paragraphs.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d09266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🧹 CLEANING DEMO: First 3 Paragraphs\n",
      "================================================================================\n",
      "\n",
      "📄 Paragraph 1 (Original):\n",
      "----------------------------------------\n",
      "ወደ ውክፔዲያ እንኳን ደህና መጡ! ማንኛውም ሰው ሊያዘጋጀው የሚችለው ነጻ መዝገበ እውቀት   ዛሬ ቅዳሜ፣ መስከረም 2 ቀን 2016 ዓ.ም. (13 ሴፕቴምበር, 2025 እ.ኤ.አ.) ነዉ።\n",
      "\n",
      "✨ Paragraph 1 (Cleaned):\n",
      "----------------------------------------\n",
      "ወደ ውክፔዲያ እንኳን ደህና መጡ ማንኛውም ሰው ሊያዘጋጀው የሚችለው ነጻ መዝገበ እውቀት ዛሬ ቅዳሜ መስከረም 2 ቀን 2016 ዓ.ም. 13 ሴፕቴምበር, 2025 እ.ኤ.አ. ነዉ\n",
      "\n",
      "🔍 Paragraph 1 (Tokens):\n",
      "----------------------------------------\n",
      "['ወደ', 'ውክፔዲያ', 'እንኳን', 'ደህና', 'መጡ', 'ማንኛውም', 'ሰው', 'ሊያዘጋጀው', 'የሚችለው', 'ነጻ']\n",
      "\n",
      "🚫 Paragraph 1 (After Stopword Removal):\n",
      "----------------------------------------\n",
      "['ወደ', 'ክፔዲያ', 'እንኳን', 'ደህና', 'መጡ', 'ማንኛውም', 'ሰው', 'ሊያዘጋጀው', 'ሚችለው', 'ነጻ']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "📄 Paragraph 2 (Original):\n",
      "----------------------------------------\n",
      "ከዚያን ጊዜ ጀምሮ፣ ኔንቲዶ በቪዲዮ ጌም ኢንደስትሪ ውስጥ በጣም የተሳካላቸው ኮንሶሎችን አዘጋጅቷል፣ ለምሳሌ ጌም ቦይ፣ ሱፐር ኔንቲዶ መዝናኛ ሲስተም፣ ኔንቲዶ ዲኤስ፣ ዊኢ እና ስዊች። ማሪዮ፣ አህያ ኮንግ፣ የዜልዳ አፈ ታሪክ፣ ሜትሮይድ፣ የእሳት አርማ፣ ኪርቢ፣ ስታር ፎክስ፣ ፖክሞን፣ ሱፐር ስማሽ ብሮስ፣ የእንስሳት...\n",
      "\n",
      "✨ Paragraph 2 (Cleaned):\n",
      "----------------------------------------\n",
      "ከዚያን ጊዜ ጀምሮ ኔንቲዶ በቪዲዮ ጌም ኢንደስትሪ ውስጥ በጣም የተሳካላቸው ኮንሶሎችን አዘጋጅቷል ለምሳሌ ጌም ቦይ ሱፐር ኔንቲዶ መዝናኛ ሲስተም ኔንቲዶ ዲኤስ ዊኢ እና ስዊች ማሪዮ አህያ ኮንግ የዜልዳ አፈ ታሪክ ሜትሮይድ የእሳት አርማ ኪርቢ ስታር ፎክስ ፖክሞን ሱፐር ስማሽ ብሮስ የእንስሳት መሻገር የዜኖብላድ ዜና...\n",
      "\n",
      "🔍 Paragraph 2 (Tokens):\n",
      "----------------------------------------\n",
      "['ከዚያን', 'ጊዜ', 'ጀምሮ', 'ኔንቲዶ', 'በቪዲዮ', 'ጌም', 'ኢንደስትሪ', 'ውስጥ', 'በጣም', 'የተሳካላቸው']\n",
      "\n",
      "🚫 Paragraph 2 (After Stopword Removal):\n",
      "----------------------------------------\n",
      "['ዚያን', 'ጊዜ', 'ጀምሮ', 'ኔንቲዶ', 'ቪዲዮ', 'ጌም', 'ኢንደስትሪ', 'ስጥ', 'ጣም', 'ተሳካላቸው']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "📄 Paragraph 3 (Original):\n",
      "----------------------------------------\n",
      "ውክፔዲያ ዓለም-ዓቀፍ  የዕውቀት  ማከማቻ ድረ-ገጽ ነው።  ውክፔዲያ የጋራ ነው።  ሁሉም ሰው በመዝገበ ዕውቀቱ  እንዲሳተፍ ተፈቅዷል።  አዳዲስ ተሳታፊዎች፣  የጀማሪዎች  ማያያዣን  በመጫን በፍጥነት ስለ ድረ-ገጹ  መረጃ ማግኘት ይችላሉ።  አዲስ ጽሑፍ ለመጻፍ ወይንም ነባር ጽሑፍ ለማስተካከል ደፍሮ መሞከር ጥሩ ል...\n",
      "\n",
      "✨ Paragraph 3 (Cleaned):\n",
      "----------------------------------------\n",
      "ውክፔዲያ ዓለም-ዓቀፍ የዕውቀት ማከማቻ ድረ-ገጽ ነው ውክፔዲያ የጋራ ነው ሁሉም ሰው በመዝገበ ዕውቀቱ እንዲሳተፍ ተፈቅዷል አዳዲስ ተሳታፊዎች የጀማሪዎች ማያያዣን በመጫን በፍጥነት ስለ ድረ-ገጹ መረጃ ማግኘት ይችላሉ አዲስ ጽሑፍ ለመጻፍ ወይንም ነባር ጽሑፍ ለማስተካከል ደፍሮ መሞከር ጥሩ ልምድ ነው ከሌሎች ውክፔዲያ...\n",
      "\n",
      "🔍 Paragraph 3 (Tokens):\n",
      "----------------------------------------\n",
      "['ውክፔዲያ', 'ዓለም-ዓቀፍ', 'የዕውቀት', 'ማከማቻ', 'ድረ-ገጽ', 'ነው', 'ውክፔዲያ', 'የጋራ', 'ነው', 'ሁሉም']\n",
      "\n",
      "🚫 Paragraph 3 (After Stopword Removal):\n",
      "----------------------------------------\n",
      "['ክፔዲያ', 'ዓለም-ዓቀፍ', 'ዕውቀት', 'ማከማቻ', 'ድረ-ገጽ', 'ክፔዲያ', 'ጋራ', 'ሁሉም', 'ሰው', 'መዝገበ']\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"🧹 CLEANING DEMO: First 3 Paragraphs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, paragraph in enumerate(corpus[:3]):\n",
    "    print(f\"\\n📄 Paragraph {i+1} (Original):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(paragraph[:200] + \"...\" if len(paragraph) > 200 else paragraph)\n",
    "\n",
    "    print(f\"\\n✨ Paragraph {i+1} (Cleaned):\")\n",
    "    print(\"-\" * 40)\n",
    "    cleaned = clean_text(paragraph)\n",
    "    print(cleaned[:200] + \"...\" if len(cleaned) > 200 else cleaned)\n",
    "\n",
    "    print(f\"\\n🔍 Paragraph {i+1} (Tokens):\")\n",
    "    print(\"-\" * 40)\n",
    "    tokens = tokenize(cleaned)\n",
    "    print(tokens[:10])  # Show first 10 tokens\n",
    "\n",
    "    print(f\"\\n🚫 Paragraph {i+1} (After Stopword Removal):\")\n",
    "    print(\"-\" * 40)\n",
    "    no_stops = remove_stopwords(tokens)\n",
    "    print(no_stops[:10])  # Show first 10 tokens\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a675e58",
   "metadata": {},
   "source": [
    "### **🧠 Load Pretrained Word Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35afc2a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_embeddings' from 'nlpta' (/workspaces/nlpta/nlpta/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnlpta\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_embeddings\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m🧠 LOADING EMBEDDINGS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'load_embeddings' from 'nlpta' (/workspaces/nlpta/nlpta/__init__.py)"
     ]
    }
   ],
   "source": [
    "from nlpta import load_embeddings\n",
    "\n",
    "print(\"\\n🧠 LOADING EMBEDDINGS\")\n",
    "print(\"=\" * 80)\n",
    "model = load_embeddings()\n",
    "print(f\"✅ Embeddings loaded with {len(model.wv)} words in vocabulary.\")\n",
    "\n",
    "# Show similarity\n",
    "test_words = [\"ኢትዮጵያ\", \"አበባ\", \"ሕዝብ\", \"መንግሥት\"]\n",
    "for word in test_words:\n",
    "    if word in model.wv.key_to_index:\n",
    "        vec = model.wv[word]\n",
    "        print(f\"  {word}: {vec[:5]}...\")  # Show first 5 dimensions\n",
    "\n",
    "if \"ኢትዮጵያ\" in model.wv.key_to_index and \"አበባ\" in model.wv.key_to_index:\n",
    "    similarity = model.wv.similarity(\"ኢትዮጵያ\", \"አበባ\")\n",
    "    print(f\"\\n📈 Similarity between 'ኢትዮጵያ' and 'አበባ': {similarity:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
