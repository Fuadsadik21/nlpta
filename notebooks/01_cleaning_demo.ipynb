{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06becde8",
   "metadata": {},
   "source": [
    "## **üßπ NLPTA: Amharic Text Cleaning, Tokenization & Stopword Removal Demo** ##\n",
    " \n",
    "\n",
    " A step-by-step walkthrough of preprocessing real Amharic text.\n",
    " \n",
    " **This notebook demonstrates:**\n",
    " - Loading sample Amharic text from Wikipedia\n",
    " - Cleaning punctuation and whitespace\n",
    " - Tokenizing into words\n",
    " - Removing stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a914ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspaces/nlpta\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/workspaces/nlpta\")\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "from nlpta import load_sample_corpus, clean_text, tokenize, remove_stopwords, load_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea0751a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading sample Amharic corpus...\n",
      "‚úÖ Loaded 4 paragraphs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"üì• Loading sample Amharic corpus...\")\n",
    "corpus = load_sample_corpus()\n",
    "print(f\"‚úÖ Loaded {len(corpus)} paragraphs.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d09266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üßπ CLEANING DEMO: First 3 Paragraphs\n",
      "================================================================================\n",
      "\n",
      "üìÑ Paragraph 1 (Original):\n",
      "----------------------------------------\n",
      "·ãà·ã∞ ·ãç·ä≠·çî·ã≤·ã´ ·ä•·äï·ä≥·äï ·ã∞·àÖ·äì ·àò·å°! ·àõ·äï·äõ·ãç·àù ·à∞·ãç ·àä·ã´·ãò·åã·åÄ·ãç ·ã®·àö·âΩ·àà·ãç ·äê·åª ·àò·ãù·åà·â† ·ä•·ãç·âÄ·âµ   ·ãõ·à¨ ·âÖ·ã≥·àú·ç£ ·àò·àµ·ä®·à®·àù 2 ·âÄ·äï 2016 ·ãì.·àù. (13 ·à¥·çï·â¥·àù·â†·à≠, 2025 ·ä•.·ä§.·ä†.) ·äê·ãâ·ç¢\n",
      "\n",
      "‚ú® Paragraph 1 (Cleaned):\n",
      "----------------------------------------\n",
      "·ãà·ã∞ ·ãç·ä≠·çî·ã≤·ã´ ·ä•·äï·ä≥·äï ·ã∞·àÖ·äì ·àò·å° ·àõ·äï·äõ·ãç·àù ·à∞·ãç ·àä·ã´·ãò·åã·åÄ·ãç ·ã®·àö·âΩ·àà·ãç ·äê·åª ·àò·ãù·åà·â† ·ä•·ãç·âÄ·âµ ·ãõ·à¨ ·âÖ·ã≥·àú ·àò·àµ·ä®·à®·àù 2 ·âÄ·äï 2016 ·ãì.·àù. 13 ·à¥·çï·â¥·àù·â†·à≠, 2025 ·ä•.·ä§.·ä†. ·äê·ãâ\n",
      "\n",
      "üîç Paragraph 1 (Tokens):\n",
      "----------------------------------------\n",
      "['·ãà·ã∞', '·ãç·ä≠·çî·ã≤·ã´', '·ä•·äï·ä≥·äï', '·ã∞·àÖ·äì', '·àò·å°', '·àõ·äï·äõ·ãç·àù', '·à∞·ãç', '·àä·ã´·ãò·åã·åÄ·ãç', '·ã®·àö·âΩ·àà·ãç', '·äê·åª']\n",
      "\n",
      "üö´ Paragraph 1 (After Stopword Removal):\n",
      "----------------------------------------\n",
      "['·ãà·ã∞', '·ä≠·çî·ã≤·ã´', '·ä•·äï·ä≥·äï', '·ã∞·àÖ·äì', '·àò·å°', '·àõ·äï·äõ·ãç·àù', '·à∞·ãç', '·àä·ã´·ãò·åã·åÄ·ãç', '·àö·âΩ·àà·ãç', '·äê·åª']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÑ Paragraph 2 (Original):\n",
      "----------------------------------------\n",
      "·ä®·ãö·ã´·äï ·åä·ãú ·åÄ·àù·àÆ·ç£ ·äî·äï·â≤·ã∂ ·â†·â™·ã≤·ãÆ ·åå·àù ·ä¢·äï·ã∞·àµ·âµ·à™ ·ãç·àµ·å• ·â†·å£·àù ·ã®·â∞·à≥·ä´·àã·â∏·ãç ·äÆ·äï·à∂·àé·âΩ·äï ·ä†·ãò·åã·åÖ·â∑·àç·ç£ ·àà·àù·à≥·àå ·åå·àù ·â¶·ã≠·ç£ ·à±·çê·à≠ ·äî·äï·â≤·ã∂ ·àò·ãù·äì·äõ ·à≤·àµ·â∞·àù·ç£ ·äî·äï·â≤·ã∂ ·ã≤·ä§·àµ·ç£ ·ãä·ä¢ ·ä•·äì ·àµ·ãä·âΩ·ç¢ ·àõ·à™·ãÆ·ç£ ·ä†·àÖ·ã´ ·äÆ·äï·åç·ç£ ·ã®·ãú·àç·ã≥ ·ä†·çà ·â≥·à™·ä≠·ç£ ·àú·âµ·àÆ·ã≠·ãµ·ç£ ·ã®·ä•·à≥·âµ ·ä†·à≠·àõ·ç£ ·ä™·à≠·â¢·ç£ ·àµ·â≥·à≠ ·çé·ä≠·àµ·ç£ ·çñ·ä≠·àû·äï·ç£ ·à±·çê·à≠ ·àµ·àõ·àΩ ·â•·àÆ·àµ·ç£ ·ã®·ä•·äï·àµ·à≥·âµ...\n",
      "\n",
      "‚ú® Paragraph 2 (Cleaned):\n",
      "----------------------------------------\n",
      "·ä®·ãö·ã´·äï ·åä·ãú ·åÄ·àù·àÆ ·äî·äï·â≤·ã∂ ·â†·â™·ã≤·ãÆ ·åå·àù ·ä¢·äï·ã∞·àµ·âµ·à™ ·ãç·àµ·å• ·â†·å£·àù ·ã®·â∞·à≥·ä´·àã·â∏·ãç ·äÆ·äï·à∂·àé·âΩ·äï ·ä†·ãò·åã·åÖ·â∑·àç ·àà·àù·à≥·àå ·åå·àù ·â¶·ã≠ ·à±·çê·à≠ ·äî·äï·â≤·ã∂ ·àò·ãù·äì·äõ ·à≤·àµ·â∞·àù ·äî·äï·â≤·ã∂ ·ã≤·ä§·àµ ·ãä·ä¢ ·ä•·äì ·àµ·ãä·âΩ ·àõ·à™·ãÆ ·ä†·àÖ·ã´ ·äÆ·äï·åç ·ã®·ãú·àç·ã≥ ·ä†·çà ·â≥·à™·ä≠ ·àú·âµ·àÆ·ã≠·ãµ ·ã®·ä•·à≥·âµ ·ä†·à≠·àõ ·ä™·à≠·â¢ ·àµ·â≥·à≠ ·çé·ä≠·àµ ·çñ·ä≠·àû·äï ·à±·çê·à≠ ·àµ·àõ·àΩ ·â•·àÆ·àµ ·ã®·ä•·äï·àµ·à≥·âµ ·àò·àª·åà·à≠ ·ã®·ãú·äñ·â•·àã·ãµ ·ãú·äì...\n",
      "\n",
      "üîç Paragraph 2 (Tokens):\n",
      "----------------------------------------\n",
      "['·ä®·ãö·ã´·äï', '·åä·ãú', '·åÄ·àù·àÆ', '·äî·äï·â≤·ã∂', '·â†·â™·ã≤·ãÆ', '·åå·àù', '·ä¢·äï·ã∞·àµ·âµ·à™', '·ãç·àµ·å•', '·â†·å£·àù', '·ã®·â∞·à≥·ä´·àã·â∏·ãç']\n",
      "\n",
      "üö´ Paragraph 2 (After Stopword Removal):\n",
      "----------------------------------------\n",
      "['·ãö·ã´·äï', '·åä·ãú', '·åÄ·àù·àÆ', '·äî·äï·â≤·ã∂', '·â™·ã≤·ãÆ', '·åå·àù', '·ä¢·äï·ã∞·àµ·âµ·à™', '·àµ·å•', '·å£·àù', '·â∞·à≥·ä´·àã·â∏·ãç']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìÑ Paragraph 3 (Original):\n",
      "----------------------------------------\n",
      "·ãç·ä≠·çî·ã≤·ã´ ·ãì·àà·àù-·ãì·âÄ·çç  ·ã®·ãï·ãç·âÄ·âµ  ·àõ·ä®·àõ·âª ·ãµ·à®-·åà·åΩ ·äê·ãç·ç¢  ·ãç·ä≠·çî·ã≤·ã´ ·ã®·åã·à´ ·äê·ãç·ç¢  ·àÅ·àâ·àù ·à∞·ãç ·â†·àò·ãù·åà·â† ·ãï·ãç·âÄ·â±  ·ä•·äï·ã≤·à≥·â∞·çç ·â∞·çà·âÖ·ã∑·àç·ç¢  ·ä†·ã≥·ã≤·àµ ·â∞·à≥·â≥·çä·ãé·âΩ·ç£  ·ã®·åÄ·àõ·à™·ãé·âΩ  ·àõ·ã´·ã´·ã£·äï  ·â†·àò·å´·äï ·â†·çç·å•·äê·âµ ·àµ·àà ·ãµ·à®-·åà·åπ  ·àò·à®·åÉ ·àõ·åç·äò·âµ ·ã≠·âΩ·àã·àâ·ç¢  ·ä†·ã≤·àµ ·åΩ·àë·çç ·àà·àò·åª·çç ·ãà·ã≠·äï·àù ·äê·â£·à≠ ·åΩ·àë·çç ·àà·àõ·àµ·â∞·ä´·ä®·àç ·ã∞·çç·àÆ ·àò·àû·ä®·à≠ ·å•·à© ·àç...\n",
      "\n",
      "‚ú® Paragraph 3 (Cleaned):\n",
      "----------------------------------------\n",
      "·ãç·ä≠·çî·ã≤·ã´ ·ãì·àà·àù-·ãì·âÄ·çç ·ã®·ãï·ãç·âÄ·âµ ·àõ·ä®·àõ·âª ·ãµ·à®-·åà·åΩ ·äê·ãç ·ãç·ä≠·çî·ã≤·ã´ ·ã®·åã·à´ ·äê·ãç ·àÅ·àâ·àù ·à∞·ãç ·â†·àò·ãù·åà·â† ·ãï·ãç·âÄ·â± ·ä•·äï·ã≤·à≥·â∞·çç ·â∞·çà·âÖ·ã∑·àç ·ä†·ã≥·ã≤·àµ ·â∞·à≥·â≥·çä·ãé·âΩ ·ã®·åÄ·àõ·à™·ãé·âΩ ·àõ·ã´·ã´·ã£·äï ·â†·àò·å´·äï ·â†·çç·å•·äê·âµ ·àµ·àà ·ãµ·à®-·åà·åπ ·àò·à®·åÉ ·àõ·åç·äò·âµ ·ã≠·âΩ·àã·àâ ·ä†·ã≤·àµ ·åΩ·àë·çç ·àà·àò·åª·çç ·ãà·ã≠·äï·àù ·äê·â£·à≠ ·åΩ·àë·çç ·àà·àõ·àµ·â∞·ä´·ä®·àç ·ã∞·çç·àÆ ·àò·àû·ä®·à≠ ·å•·à© ·àç·àù·ãµ ·äê·ãç ·ä®·àå·àé·âΩ ·ãç·ä≠·çî·ã≤·ã´...\n",
      "\n",
      "üîç Paragraph 3 (Tokens):\n",
      "----------------------------------------\n",
      "['·ãç·ä≠·çî·ã≤·ã´', '·ãì·àà·àù-·ãì·âÄ·çç', '·ã®·ãï·ãç·âÄ·âµ', '·àõ·ä®·àõ·âª', '·ãµ·à®-·åà·åΩ', '·äê·ãç', '·ãç·ä≠·çî·ã≤·ã´', '·ã®·åã·à´', '·äê·ãç', '·àÅ·àâ·àù']\n",
      "\n",
      "üö´ Paragraph 3 (After Stopword Removal):\n",
      "----------------------------------------\n",
      "['·ä≠·çî·ã≤·ã´', '·ãì·àà·àù-·ãì·âÄ·çç', '·ãï·ãç·âÄ·âµ', '·àõ·ä®·àõ·âª', '·ãµ·à®-·åà·åΩ', '·ä≠·çî·ã≤·ã´', '·åã·à´', '·àÅ·àâ·àù', '·à∞·ãç', '·àò·ãù·åà·â†']\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"üßπ CLEANING DEMO: First 3 Paragraphs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, paragraph in enumerate(corpus[:3]):\n",
    "    print(f\"\\nüìÑ Paragraph {i+1} (Original):\")\n",
    "    print(\"-\" * 40)\n",
    "    print(paragraph[:200] + \"...\" if len(paragraph) > 200 else paragraph)\n",
    "\n",
    "    print(f\"\\n‚ú® Paragraph {i+1} (Cleaned):\")\n",
    "    print(\"-\" * 40)\n",
    "    cleaned = clean_text(paragraph)\n",
    "    print(cleaned[:200] + \"...\" if len(cleaned) > 200 else cleaned)\n",
    "\n",
    "    print(f\"\\nüîç Paragraph {i+1} (Tokens):\")\n",
    "    print(\"-\" * 40)\n",
    "    tokens = tokenize(cleaned)\n",
    "    print(tokens[:10])  # Show first 10 tokens\n",
    "\n",
    "    print(f\"\\nüö´ Paragraph {i+1} (After Stopword Removal):\")\n",
    "    print(\"-\" * 40)\n",
    "    no_stops = remove_stopwords(tokens)\n",
    "    print(no_stops[:10])  # Show first 10 tokens\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a675e58",
   "metadata": {},
   "source": [
    "### **üß† Load Pretrained Word Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35afc2a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_embeddings' from 'nlpta' (/workspaces/nlpta/nlpta/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnlpta\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_embeddings\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müß† LOADING EMBEDDINGS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'load_embeddings' from 'nlpta' (/workspaces/nlpta/nlpta/__init__.py)"
     ]
    }
   ],
   "source": [
    "from nlpta import load_embeddings\n",
    "\n",
    "print(\"\\nüß† LOADING EMBEDDINGS\")\n",
    "print(\"=\" * 80)\n",
    "model = load_embeddings()\n",
    "print(f\"‚úÖ Embeddings loaded with {len(model.wv)} words in vocabulary.\")\n",
    "\n",
    "# Show similarity\n",
    "test_words = [\"·ä¢·âµ·ãÆ·åµ·ã´\", \"·ä†·â†·â£\", \"·àï·ãù·â•\", \"·àò·äï·åç·à•·âµ\"]\n",
    "for word in test_words:\n",
    "    if word in model.wv.key_to_index:\n",
    "        vec = model.wv[word]\n",
    "        print(f\"  {word}: {vec[:5]}...\")  # Show first 5 dimensions\n",
    "\n",
    "if \"·ä¢·âµ·ãÆ·åµ·ã´\" in model.wv.key_to_index and \"·ä†·â†·â£\" in model.wv.key_to_index:\n",
    "    similarity = model.wv.similarity(\"·ä¢·âµ·ãÆ·åµ·ã´\", \"·ä†·â†·â£\")\n",
    "    print(f\"\\nüìà Similarity between '·ä¢·âµ·ãÆ·åµ·ã´' and '·ä†·â†·â£': {similarity:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
