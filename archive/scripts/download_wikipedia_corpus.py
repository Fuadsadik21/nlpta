import wikipediaapi
import os

def download_wikipedia_corpus():
    """Download large corpus from Amharic Wikipedia."""
    wiki = wikipediaapi.Wikipedia(
        language='am',
        user_agent='nlpta-crawler (https://github.com/fuadsadik21/nlpta)'
    )
    
    # Get list of pages (you can expand this list)
    pages = [
        "áŠ¢á‰µá‹®áŒµá‹«", "áŠ á‹²áˆµ áŠ á‰ á‰£", "áŠ áˆ›áˆ­áŠ›", "áŠ¦áˆ®áˆ", "á‰µáŒáˆ«á‹­", "áˆ¶áˆ›áˆŠ", "áŠ á‹áˆ­",
        "áŒáŒƒáˆ", "áˆ»á‹‹", "á‹ˆáˆ", "á‰¤á‰° áŠ­áˆ­áˆµá‰²á‹«áŠ•", "áŠ¥áˆµáˆáˆáŠ“", "áŒáˆ³", "á‰µáˆáˆ…áˆ­á‰µ", "áŒ¤áŠ“",
        "áŠ¢áŠ®áŠ–áˆš", "á–áˆˆá‰²áŠ«", "á‰³áˆªáŠ­", "á‰£áˆ…áˆ", "áˆ™á‹šá‰ƒ", "áŠáˆáˆ", "áˆµá–áˆ­á‰µ", "á‰´áŠ­áŠ–áˆáŒ‚"
    ]
    
    all_text = []
    for page_title in pages:
        print(f"ğŸ“¥ Fetching: {page_title}")
        page = wiki.page(page_title)
        if page.exists():
            all_text.append(page.text)
        else:
            print(f"  âŒ Page not found: {page_title}")
    
    os.makedirs("data", exist_ok=True)
    with open("data/wikipedia_full_corpus.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(all_text))
    
    print(f"âœ… Saved {len(pages)} Wikipedia pages.")

if __name__ == "__main__":
    download_wikipedia_corpus()